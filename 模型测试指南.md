# 🧪 大模型测试指南

## 📋 测试准备

### 1. 配置 API Keys

在 `.env.local` 文件中添加各个大模型的 API Key：

```bash
# 智谱AI
ZHIPU_API_KEY=your_zhipu_api_key

# 百度千帆
BAIDU_API_KEY=your_baidu_api_key
BAIDU_SECRET_KEY=your_baidu_secret_key

# 豆包（字节跳动）
DOUBAO_API_KEY=your_doubao_api_key

# DeepSeek
DEEPSEEK_API_KEY=your_deepseek_api_key

# 文心一言
WENXIN_API_KEY=your_wenxin_api_key
WENXIN_SECRET_KEY=your_wenxin_secret_key
```

### 2. 准备测试数据

测试数据已保存在 `test-data.json` 文件中，包含：
- 多音字测试（中、重、解、差、切、奇等）
- 儿化音测试（花儿、小猫儿）
- 英文单词测试（apple、the、record等）
- 特殊符号和缩写（PDF、NASA等）

你可以根据需要修改或扩展测试数据。

## 🚀 运行测试

### 方法 1：使用 Node.js 直接运行

```bash
# 安装依赖（如果还没有）
npm install

# 运行测试脚本
npx ts-node scripts/test-models.ts
```

### 方法 2：创建测试 API 路由

创建一个 API 路由来运行测试：

```bash
# 在 app/api/test-models/route.ts 中创建测试端点
```

然后通过浏览器或 curl 访问：
```bash
curl http://localhost:3001/api/test-models
```

### 方法 3：使用 Next.js API 路由

我已经创建了测试脚本，你可以：

1. **创建测试 API 路由**（推荐）：
   - 创建 `app/api/test-models/route.ts`
   - 调用测试函数
   - 通过浏览器访问测试

2. **或者直接运行脚本**：
   ```bash
   npx tsx scripts/test-models.ts
   ```

## 📊 测试结果

测试完成后会生成：

1. **控制台输出**：实时显示测试进度和结果
2. **test-report.txt**：详细的文本报告
3. **test-results.json**：JSON格式的测试结果

### 报告内容

- **模型性能汇总**：准确率、平均耗时、通过/总数
- **详细测试结果**：每个测试用例的输入、期望、实际结果

## 🔍 评估指标

### 1. 准确率（Accuracy）
- 计算方式：正确标注的测试用例数 / 总测试用例数
- 权重：最重要指标

### 2. 平均耗时（Average Time）
- 计算方式：所有测试用例的平均响应时间
- 单位：毫秒（ms）

### 3. 错误率（Error Rate）
- 计算方式：API调用失败的测试用例数 / 总测试用例数

## 📝 测试用例说明

测试数据集包含以下类型的用例：

1. **多音字测试**（1-20）：
   - 中、重、解、差、切、奇等常见多音字
   - 测试模型是否能根据上下文正确判断读音

2. **儿化音测试**（9-10）：
   - 花儿、小猫儿等儿化音
   - 测试模型是否能正确处理儿化音标注

3. **英文单词测试**（21-23）：
   - apple、the、record等英文单词
   - 测试模型是否能正确标注英文音标

4. **特殊符号测试**（24-51）：
   - PDF、NASA、URL、WiFi等缩写
   - 数字和符号转换
   - 测试模型是否能处理特殊格式

## 🎯 选择最佳模型

根据测试结果，选择最佳模型时考虑：

1. **准确率优先**：选择准确率最高的模型
2. **速度优先**：如果准确率相近，选择响应时间最短的
3. **成本考虑**：考虑API调用成本
4. **稳定性**：选择错误率最低的模型

## 🔧 自定义测试

### 添加新的测试用例

编辑 `test-data.json`，添加新的测试用例：

```json
{
  "id": 24,
  "input": "你的测试文本",
  "expected": "期望的标注结果"
}
```

### 添加新的模型

1. 在 `lib/` 目录下创建新的模型文件（如 `lib/newmodel.ts`）
2. 实现 `callNewModelAPI` 函数
3. 在 `scripts/test-models.ts` 中添加模型到测试列表

### 修改评估标准

在 `scripts/test-models.ts` 中修改 `compareResults` 函数来自定义比较逻辑。

## 📈 持续测试

建议：

1. **定期测试**：每次更新 Prompt 或规则后重新测试
2. **版本对比**：保存不同版本的测试结果进行对比
3. **A/B测试**：同时运行多个模型，对比实时效果

## 🆘 常见问题

### Q: API Key 配置错误
A: 检查 `.env.local` 文件中的 API Key 是否正确，确保没有多余的空格或引号。

### Q: API 调用失败
A: 检查网络连接、API Key 是否有效、是否有足够的额度。

### Q: 测试结果不一致
A: 大模型的输出可能有随机性，建议多次测试取平均值。

### Q: 如何添加更多测试用例
A: 编辑 `test-data.json` 文件，按照现有格式添加新的测试用例。

## 📚 相关文档

- [智谱AI API 文档](https://open.bigmodel.cn/)
- [百度千帆 API 文档](https://cloud.baidu.com/product/wenxinworkshop)
- [DeepSeek API 文档](https://platform.deepseek.com/api-docs/)
- [豆包 API 文档](https://www.volcengine.com/docs/82379)


